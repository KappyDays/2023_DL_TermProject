f1-score:
              precision    recall  f1-score   support

           0     0.9205    0.9643    0.9419       168
           1     0.9936    0.9630    0.9781       162
           2     0.9911    0.9962    0.9937       786
           3     0.8743    0.9330    0.9027       179
           4     0.9571    0.8933    0.9241       150
           5     0.9431    0.9005    0.9213       221
           6     0.9157    0.9760    0.9449       167
           7     0.7479    0.9531    0.8381       277
           8     0.9728    0.9728    0.9728       147
           9     0.8207    0.8483    0.8343       178
          10     0.9960    0.9920    0.9940       249
          11     0.9216    0.9463    0.9338       149
          12     0.9167    0.6667    0.7719        99
          13     0.9911    0.9328    0.9610       119
          14     0.9339    0.9860    0.9593       215
          15     0.9732    0.9121    0.9417       239
          16     0.9244    0.9322    0.9283       118
          17     0.8571    0.2069    0.3333       116
          18     0.8874    0.8995    0.8934       219
          19     0.9630    0.8844    0.9220       147
          20     0.7866    0.9773    0.8716       132
          21     0.9697    0.9796    0.9746       196
          22     0.9238    0.9838    0.9528       308
          23     0.8652    0.9747    0.9167        79
          24     0.8021    0.9494    0.8696       158
          25     0.9783    0.9730    0.9756       185
          26     0.9655    0.8796    0.9205       191
          27     0.9562    0.9424    0.9493       139
          28     0.9550    0.9680    0.9615       219
          29     0.9707    0.9851    0.9779       202
          30     0.9932    0.8391    0.9097       174
          31     0.9573    0.9573    0.9573       164
          32     0.8982    0.9202    0.9091       163
          33     0.7594    0.9817    0.8564       164
          34     0.9245    0.9671    0.9453       152
          35     0.9814    0.9017    0.9399       234
          36     0.9836    0.7500    0.8511       160
          37     0.9091    0.8772    0.8929        57
          38     0.9588    0.9760    0.9674       167
          39     0.8800    0.7904    0.8328       167

    accuracy                         0.9255      7516
   macro avg     0.9230    0.9083    0.9081      7516
weighted avg     0.9298    0.9255    0.9223      7516
         
class_dict: {'가는장구채': 0, '가시박': 1, '가지': 2, '개여뀌': 3, '개오동': 4, '개옻나무': 5, '곰취': 6, '까마중': 7, '꽃개오동': 8, '능소화': 9, '닥풀': 10, '담배풀': 11, '대청부채': 12, '도깨비가지': 13, '독미나리': 14, '맑은대쑥': 15, '묏미나리': 16, '미국까마중': 17, '미국능소화': 18, '미나리': 19, '범부채': 20, '분홍장구채': 21, '붉나무': 22, '술패랭이꽃': 23, '아까시나무': 24, '어저귀': 25, '여뀌': 26, '여우오줌': 27, '왕자귀나무': 28, '자귀나무': 29, '장구채': 30, '제비쑥': 31, '좀담배풀': 32, '진득찰': 33, '쪽': 34, '참취': 35, '털진득찰': 36, '패랭이꽃': 37, '하늘타리': 38, '회화나무': 39},         
class_acc: tensor([0.9643, 0.9630, 0.9962, 0.9330, 0.8933, 0.9005, 0.9760, 0.9531, 0.9728,
        0.8483, 0.9920, 0.9463, 0.6667, 0.9328, 0.9860, 0.9121, 0.9322, 0.2069,
        0.8995, 0.8844, 0.9773, 0.9796, 0.9838, 0.9747, 0.9494, 0.9730, 0.8796,
        0.9424, 0.9680, 0.9851, 0.8391, 0.9573, 0.9202, 0.9817, 0.9671, 0.9017,
        0.7500, 0.8772, 0.9760, 0.7904]),
         
Total Test Accurcay: 92.54922831293241%

